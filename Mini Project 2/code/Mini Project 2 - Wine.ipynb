{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP-551 Mini-Project #2.\n",
    "# Written by Ryan Wilson.\n",
    "# Encoding = UTF-8.\n",
    "\n",
    "# Import pandas, numpy and matplotlib.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import preprocessing & classifier modules from scikit-learn.\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Request:\n",
      "  This dataset is public available for research. The details are described in [Cortez et al., 2009]. \n",
      "  Please include this citation if you plan to use this database:\n",
      "\n",
      "  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
      "  Modeling wine preferences by data mining from physicochemical properties.\n",
      "  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
      "\n",
      "  Available at: [@Elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016\n",
      "                [Pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf\n",
      "                [bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib\n",
      "\n",
      "1. Title: Wine Quality \n",
      "\n",
      "2. Sources\n",
      "   Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\n",
      "   \n",
      "3. Past Usage:\n",
      "\n",
      "  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
      "  Modeling wine preferences by data mining from physicochemical properties.\n",
      "  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
      "\n",
      "  In the above reference, two datasets were created, using red and white wine samples.\n",
      "  The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n",
      "  (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n",
      "  between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model\n",
      "  these datasets under a regression approach. The support vector machine model achieved the\n",
      "  best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T),\n",
      "  etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity\n",
      "  analysis procedure).\n",
      " \n",
      "4. Relevant Information:\n",
      "\n",
      "   The two datasets are related to red and white variants of the Portuguese &quot;Vinho Verde&quot; wine.\n",
      "   For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].\n",
      "   Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables \n",
      "   are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
      "\n",
      "   These datasets can be viewed as classification or regression tasks.\n",
      "   The classes are ordered and not balanced (e.g. there are munch more normal wines than\n",
      "   excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent\n",
      "   or poor wines. Also, we are not sure if all input variables are relevant. So\n",
      "   it could be interesting to test feature selection methods. \n",
      "\n",
      "5. Number of Instances: red wine - 1599; white wine - 4898. \n",
      "\n",
      "6. Number of Attributes: 11 + output attribute\n",
      "  \n",
      "   Note: several of the attributes may be correlated, thus it makes sense to apply some sort of\n",
      "   feature selection.\n",
      "\n",
      "7. Attribute information:\n",
      "\n",
      "   For more information, read [Cortez et al., 2009].\n",
      "\n",
      "   Input variables (based on physicochemical tests):\n",
      "   1 - fixed acidity\n",
      "   2 - volatile acidity\n",
      "   3 - citric acid\n",
      "   4 - residual sugar\n",
      "   5 - chlorides\n",
      "   6 - free sulfur dioxide\n",
      "   7 - total sulfur dioxide\n",
      "   8 - density\n",
      "   9 - pH\n",
      "   10 - sulphates\n",
      "   11 - alcohol\n",
      "   Output variable (based on sensory data): \n",
      "   12 - quality (score between 0 and 10)\n",
      "\n",
      "8. Missing Attribute Values: None\n",
      "\n",
      "Downloaded from openml.org.\n",
      "\n",
      "Feature Data (X):\n",
      "       V1    V2    V3    V4     V5    V6     V7       V8    V9   V10   V11\n",
      "0     7.0  0.27  0.36  20.7  0.045  45.0  170.0  1.00100  3.00  0.45   8.8\n",
      "1     6.3  0.30  0.34   1.6  0.049  14.0  132.0  0.99400  3.30  0.49   9.5\n",
      "2     8.1  0.28  0.40   6.9  0.050  30.0   97.0  0.99510  3.26  0.44  10.1\n",
      "3     7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.99560  3.19  0.40   9.9\n",
      "4     7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.99560  3.19  0.40   9.9\n",
      "...   ...   ...   ...   ...    ...   ...    ...      ...   ...   ...   ...\n",
      "4893  6.2  0.21  0.29   1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2\n",
      "4894  6.6  0.32  0.36   8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6\n",
      "4895  6.5  0.24  0.19   1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4\n",
      "4896  5.5  0.29  0.30   1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8\n",
      "4897  6.0  0.21  0.38   0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8\n",
      "\n",
      "[4898 rows x 11 columns]\n",
      "\n",
      "Target Data (y):\n",
      "0       4.0\n",
      "1       4.0\n",
      "2       4.0\n",
      "3       4.0\n",
      "4       4.0\n",
      "       ... \n",
      "4893    4.0\n",
      "4894    3.0\n",
      "4895    4.0\n",
      "4896    5.0\n",
      "4897    4.0\n",
      "Name: Class, Length: 4898, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "##########          IMPORT & DATAFRAME INITIALIZATION          ##########\n",
    "#########################################################################\n",
    "\n",
    "# Load iris dataset from sklearn.\n",
    "data = fetch_openml(name='wine-quality-white')\n",
    "\n",
    "# Print full description of dataset.\n",
    "# pylint: disable=E1101\n",
    "print(data.DESCR)\n",
    "print()\n",
    "\n",
    "# Initialize dataframe & set feature and target arrays to X & Y.\n",
    "df = pd.DataFrame(data = np.c_[data['data'], data['target']], \\\n",
    "    columns = data['feature_names'] + ['Class'])\n",
    "#df.to_csv('d:/PhD Thesis/Courses/COMP 551/Python/Python Files/Mini-Project 2/df_wine.csv', index = True)\n",
    "\n",
    "X = df.iloc[:, :-1].astype(float)\n",
    "y = df.iloc[:, -1].astype(float)\n",
    "\n",
    "# Set up print formatting options (adjust significant digits).\n",
    "np.set_printoptions(formatter={'float': '{:0.4f}'.format})\n",
    "\n",
    "# Print dataframes to ensure proper import.\n",
    "print(\"Feature Data (X):\")\n",
    "print(X)\n",
    "print()\n",
    "\n",
    "print(\"Target Data (y):\")\n",
    "print(y)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Data X_norm [re-scaled w/ bias term]:\n",
      "      bias        V1        V2        V3        V4        V5        V6  \\\n",
      "0        1  0.002459 -0.001168  0.003047  0.040313 -0.000505  0.008144   \n",
      "1        1 -0.009395  0.003085  0.000686 -0.013499  0.002111 -0.017904   \n",
      "2        1  0.021086  0.000249  0.007771  0.001433  0.002765 -0.004460   \n",
      "3        1  0.005846 -0.006839 -0.001676  0.005941  0.007998  0.009824   \n",
      "4        1  0.005846 -0.006839 -0.001676  0.005941  0.007998  0.009824   \n",
      "...    ...       ...       ...       ...       ...       ...       ...   \n",
      "4893     1 -0.011088 -0.009675 -0.005218 -0.013499 -0.004430 -0.009502   \n",
      "4894     1 -0.004315  0.005920  0.003047  0.004532  0.000803  0.018226   \n",
      "4895     1 -0.006008 -0.005422 -0.017026 -0.014626 -0.003121 -0.004460   \n",
      "4896     1 -0.022942  0.001667 -0.004037 -0.014908 -0.015549 -0.012862   \n",
      "4897     1 -0.014475 -0.009675  0.005409 -0.015753 -0.016857 -0.011182   \n",
      "\n",
      "            V7        V8        V9       V10       V11  \n",
      "0     0.010639  0.033314 -0.017817 -0.004989 -0.019906  \n",
      "1    -0.002139 -0.000131  0.010574  0.000019 -0.011778  \n",
      "2    -0.013908  0.005125  0.006789 -0.006241 -0.004811  \n",
      "3     0.016019  0.007514  0.000164 -0.011250 -0.007133  \n",
      "4     0.016019  0.007514  0.000164 -0.011250 -0.007133  \n",
      "...        ...       ...       ...       ...       ...  \n",
      "4893 -0.015589 -0.013795  0.007735  0.001271  0.007963  \n",
      "4894  0.009966  0.004169 -0.003621 -0.003737 -0.010617  \n",
      "4895 -0.009200 -0.007106 -0.018763 -0.003737 -0.012939  \n",
      "4896 -0.009536 -0.025501  0.014359 -0.013754  0.026542  \n",
      "4897 -0.013571 -0.022061  0.006789 -0.021267  0.014930  \n",
      "\n",
      "[4898 rows x 12 columns]\n",
      "\n",
      "Target Data y_encoded [one-hot encoded]:\n",
      "      1.0  2.0  3.0  4.0  5.0  6.0  7.0\n",
      "0       0    0    0    1    0    0    0\n",
      "1       0    0    0    1    0    0    0\n",
      "2       0    0    0    1    0    0    0\n",
      "3       0    0    0    1    0    0    0\n",
      "4       0    0    0    1    0    0    0\n",
      "...   ...  ...  ...  ...  ...  ...  ...\n",
      "4893    0    0    0    1    0    0    0\n",
      "4894    0    0    1    0    0    0    0\n",
      "4895    0    0    0    1    0    0    0\n",
      "4896    0    0    0    0    1    0    0\n",
      "4897    0    0    0    1    0    0    0\n",
      "\n",
      "[4898 rows x 7 columns]\n",
      "\n",
      "X Null Values: False\n",
      "y Null Values: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########################################################################\n",
    "#################          DATA PREPROCESSING          ##################\n",
    "#########################################################################\n",
    "\n",
    "# Mean-center each variable (features & target) by column.\n",
    "X_mean = X.mean(axis=0)\n",
    "X_centered = X - X_mean\n",
    "\n",
    "# Re-scale each variable by setting L2-norm to 1 (column-wise).\n",
    "X_norm_fn = np.sqrt(np.sum(np.square(X_centered), axis=0))\n",
    "X_norm = X_centered / X_norm_fn\n",
    "\n",
    "# Insert leading bias term (w0=1) column to features dataframe.\n",
    "X_norm = pd.concat([pd.Series(1, index=X_norm.index, name='bias'), X_norm], axis=1)\n",
    "\n",
    "# One-hot encode multiclass target variable for classification algorithm.\n",
    "dummies = pd.get_dummies(y[:])\n",
    "y_encoded = pd.concat([y, dummies], axis=1)\n",
    "y_encoded = y_encoded.drop(['Class'], axis=1)\n",
    "\n",
    "#X_norm.to_csv('d:/PhD Thesis/Courses/COMP 551/Python/Python Files/Mini-Project 2/df_wine_X-norm.csv', index = True)\n",
    "#y_encoded.to_csv('d:/PhD Thesis/Courses/COMP 551/Python/Python Files/Mini-Project 2/df_wine_y-encoded.csv', index = True)\n",
    "\n",
    "# Preview the preprocessed data to ensure proper scaling.\n",
    "print(\"Feature Data X_norm [re-scaled w/ bias term]:\")\n",
    "print(X_norm)\n",
    "print()\n",
    "\n",
    "print(\"Target Data y_encoded [one-hot encoded]:\")\n",
    "print(y_encoded)\n",
    "print()\n",
    "\n",
    "# Check for null values (NaNs) generated by preprocessing.\n",
    "print(\"X Null Values:\", X.isnull().values.any())\n",
    "print(\"y Null Values:\", y.isnull().values.any())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "############          CLASS & FUNCTION DEFINITIONS          #############\n",
    "#########################################################################\n",
    "\n",
    "def activation_fn(z):\n",
    "    z -= np.max(z)\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "    #return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "class SoftmaxRegressionClassifier:\n",
    "    def __init__(self,\n",
    "            alpha=0.001,\n",
    "            epsilon=1e-5,\n",
    "            epochs=100,\n",
    "            batch_size=8,\n",
    "            beta_decay=0.9,\n",
    "            early_stop_iter=10,\n",
    "            record_history=True,\n",
    "            validation_set=(None,None)):\n",
    "        # Gradient Descent Parameters:\n",
    "        self.alpha = float(alpha)\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.beta_decay = float(beta_decay)\n",
    "        self.early_stop_iter = int(early_stop_iter)\n",
    "        self.record_history = record_history\n",
    "        if record_history:\n",
    "            self.training_loss_history = []\n",
    "        self.X_validation, self.y_validation = validation_set\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Determine the shape of input matrix X.\n",
    "        N, D = X.shape\n",
    "        K = len(y.columns)\n",
    "        \n",
    "        # Set up the convergence check.\n",
    "        previous_loss = -float('inf')\n",
    "        self.converged = False\n",
    "        self.stopped_early = False\n",
    "        \n",
    "        # Initialize the fit parameters.\n",
    "        self.theta = np.zeros((D, K))\n",
    "        beta = self.theta * 0\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            perm_fn = np.random.permutation(len(y))\n",
    "            X = X.iloc[perm_fn, :]\n",
    "            y = y.iloc[perm_fn]\n",
    "\n",
    "            # Allow final batch to contain less samples (if necessary).\n",
    "            batchling = (1 if N % self.batch_size else 0)\n",
    "\n",
    "            # Divide X & y datasets into mini-batches.\n",
    "            for batch_index in range(N // self.batch_size + batchling):\n",
    "                mini_batch = slice(self.batch_size * batch_index, self.batch_size * (batch_index + 1))\n",
    "                X_batch = X.iloc[mini_batch, :]\n",
    "                y_batch = y.iloc[mini_batch]\n",
    "\n",
    "                theta_next = self.theta + self.beta_decay * beta\n",
    "                y_hat = activation_fn(X_batch @ theta_next)\n",
    "\n",
    "                # Perform gradient descent.\n",
    "                residuals = np.subtract(y_hat, y_batch)\n",
    "                gradient = X_batch.T @ residuals\n",
    "\n",
    "                beta = np.subtract((self.beta_decay * beta), (self.alpha * gradient))\n",
    "                self.theta += beta\n",
    "            \n",
    "            # Check for convergence at end of each epoch.\n",
    "            self.loss = np.mean(-1 * np.sum(y.T * (X @ self.theta)) + ((X @ self.theta).max()) \\\n",
    "                + (np.log(np.sum(np.exp((X @ self.theta) - (X @ self.theta).max())))))\n",
    "\n",
    "            #self.loss = np.mean(-1 * (np.sum(y * np.log(activation_fn(X @ self.theta)+1e-6))))\n",
    "            self.training_loss_history.append(self.loss)\n",
    "\n",
    "            # Initialize early stopping mechanism.\n",
    "            if self.check_validation_loss():\n",
    "                self.stopped_early = True\n",
    "                break\n",
    "            \n",
    "            if abs(previous_loss - self.loss) < self.epsilon:\n",
    "                self.converged = True\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = self.loss\n",
    "        \n",
    "        self.iterations = i+1\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        return activation_fn(X @ self.theta)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.predict_prob(X)\n",
    "        return self.classlabels(prob)\n",
    "\n",
    "    def classlabels(self, z):\n",
    "        return z.idxmax(axis=1)+1\n",
    "    \n",
    "    def check_validation_loss(self):\n",
    "        # Validation set loss history.\n",
    "        if not hasattr(self, 'validation_loss_history'):\n",
    "            self.validation_loss_history = []\n",
    "        \n",
    "        loss = np.mean(-1 * np.sum(self.y_validation.T * (self.X_validation @ self.theta)) \\\n",
    "            + ((self.X_validation @ self.theta).max()) + (np.log(np.sum(np.exp((self.X_validation @ self.theta) \\\n",
    "                - (self.X_validation @ self.theta).max())))))\n",
    "\n",
    "        #loss = np.mean(-1 * np.sum(self.y_validation * np.log(activation_fn(self.X_validation @ self.theta)+1e-6)))\n",
    "        self.validation_loss_history.append(loss)\n",
    "\n",
    "        # Area Under Curve (AUC) & ROC history.\n",
    "        if not hasattr(self, 'auc_history'):\n",
    "            self.auc_history = []\n",
    "        p_hat_auc = self.predict_prob(self.X_validation)\n",
    "        auc = roc_auc_score(self.y_validation, p_hat_auc, average='macro', multi_class='ovo')\n",
    "        self.auc_history.append(auc)\n",
    "\n",
    "        # Implement early stopping mechanism.\n",
    "        t = self.early_stop_iter\n",
    "\n",
    "        if t and len(self.validation_loss_history) > t * 10:\n",
    "            recent_best = min(self.validation_loss_history[-t:])\n",
    "            previous_best = min(self.validation_loss_history[:-t])\n",
    "            if recent_best > previous_best:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "############           K-FOLD CV DATA PREPARATION           #############\n",
    "#########################################################################\n",
    "\n",
    "# Shuffle/permute the X & y data (w/ equivalent indices).\n",
    "perm_fn = np.random.permutation(len(y_encoded))\n",
    "X_norm = X_norm.iloc[perm_fn, :]\n",
    "y_encoded = y_encoded.iloc[perm_fn]\n",
    "\n",
    "# Split into K-folds (randomly permuted).\n",
    "nfolds = 5\n",
    "from math import ceil\n",
    "fold_size = ceil(len(y_encoded.index) / nfolds)\n",
    "\n",
    "X_fold1, y_fold1 = X_norm.iloc[:fold_size, :], y_encoded.iloc[:fold_size, :]\n",
    "X_fold2, y_fold2 = X_norm.iloc[fold_size:fold_size*2, :], y_encoded.iloc[fold_size:fold_size*2, :]\n",
    "X_fold3, y_fold3 = X_norm.iloc[fold_size*2:fold_size*3, :], y_encoded.iloc[fold_size*2:fold_size*3, :]\n",
    "X_fold4, y_fold4 = X_norm.iloc[fold_size*3:fold_size*4, :], y_encoded.iloc[fold_size*3:fold_size*4, :]\n",
    "X_fold5, y_fold5 = X_norm.iloc[fold_size*4:, :], y_encoded.iloc[fold_size*4:, :]\n",
    "\n",
    "# Group folds into respective train & test sets.\n",
    "# Train: folds 2-3-4-5, Test: fold 1\n",
    "X_train1, y_train1 = pd.concat([X_fold2, X_fold3, X_fold4, X_fold5]), pd.concat([y_fold2, y_fold3, y_fold4, y_fold5])\n",
    "X_test1, y_test1 = X_fold1, y_fold1\n",
    "\n",
    "# Train: folds 1-3-4-5, Test: fold 2\n",
    "X_train2, y_train2 = pd.concat([X_fold1, X_fold3, X_fold4, X_fold5]), pd.concat([y_fold1, y_fold3, y_fold4, y_fold5])\n",
    "X_test2, y_test2 = X_fold2, y_fold2\n",
    "\n",
    "# Train: folds 1-2-4-5, Test: fold 3\n",
    "X_train3, y_train3 = pd.concat([X_fold1, X_fold2, X_fold4, X_fold5]), pd.concat([y_fold1, y_fold2, y_fold4, y_fold5])\n",
    "X_test3, y_test3 = X_fold3, y_fold3\n",
    "\n",
    "# Train: folds 1-2-3-5, Test: fold 4\n",
    "X_train4, y_train4 = pd.concat([X_fold1, X_fold2, X_fold3, X_fold5]), pd.concat([y_fold1, y_fold2, y_fold3, y_fold5])\n",
    "X_test4, y_test4 = X_fold4, y_fold4\n",
    "\n",
    "# Train: folds 1-2-3-4, Test: fold 5\n",
    "X_train5, y_train5 = pd.concat([X_fold1, X_fold2, X_fold3, X_fold4]), pd.concat([y_fold1, y_fold2, y_fold3, y_fold4])\n",
    "X_test5, y_test5 = X_fold5, y_fold5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "############          MODEL TESTING & PERFORMANCE           #############\n",
    "#########################################################################\n",
    "\n",
    "# Initialize arrays to pull data for future calculations.\n",
    "\n",
    "alpha = [0.1, 0.01, 0.001]\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "RMSE = []\n",
    "\n",
    "for a in range(0, nfolds):\n",
    "    train_accuracy2 = []\n",
    "    train_accuracy.append(train_accuracy2)\n",
    "\n",
    "for b in range(0, nfolds):\n",
    "    test_accuracy2 = []\n",
    "    test_accuracy.append(test_accuracy2)\n",
    "\n",
    "for c in range(0, nfolds):\n",
    "    RMSE2 = []\n",
    "    RMSE.append(RMSE2)\n",
    "\n",
    "# Fit & train softmax regression model using 5-fold CV.\n",
    "for i in range(1, nfolds+1):\n",
    "    X_train = globals()[f\"X_train{i}\"]\n",
    "    X_test = globals()[f\"X_test{i}\"]\n",
    "    y_train = globals()[f\"y_train{i}\"]\n",
    "    y_test = globals()[f\"y_test{i}\"]\n",
    "\n",
    "    # Loop over 'K' values.\n",
    "    for j, k in enumerate(alpha):\n",
    "        model = SoftmaxRegressionClassifier(alpha=k, validation_set=(X_test, y_test))\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Model Stats (converged, stopped, iterations, loss):\")\n",
    "        print(model.converged, model.stopped_early, model.iterations, model.loss)\n",
    "        print()\n",
    "\n",
    "        # Compute predictions & test scores.\n",
    "        p_hat_train = model.predict_prob(X_train)\n",
    "        y_hat_train = model.predict(X_train)\n",
    "\n",
    "        p_hat = model.predict_prob(X_test)\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        # Compute & store training/test scores & RMSE.\n",
    "        train_accuracy_temp = accuracy_score(model.classlabels(y_train)-1, y_hat_train)\n",
    "        train_accuracy[i-1].append(train_accuracy_temp)\n",
    "        \n",
    "        test_accuracy_temp = accuracy_score(model.classlabels(y_test)-1, y_hat)\n",
    "        test_accuracy[i-1].append(test_accuracy_temp)\n",
    "\n",
    "        RMSE_temp = np.sqrt(np.mean(np.square(np.subtract(y_hat, model.classlabels(y_test)-1))))\n",
    "        RMSE[i-1].append(RMSE_temp)\n",
    "\n",
    "        # Print training/test scores & RMSE.\n",
    "        print(\"Training Score (Accuracy):\")\n",
    "        print(train_accuracy_temp)\n",
    "        print()\n",
    "        \n",
    "        print(\"Test Score (Accuracy):\")\n",
    "        print(test_accuracy_temp)\n",
    "        print()\n",
    "\n",
    "        print(\"Root Mean Square Error (RMSE):\")\n",
    "        print(RMSE_temp)\n",
    "        print()\n",
    "\n",
    "        # Compute & print confusion matrix.\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(model.classlabels(y_test)-1, y_hat))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    #########################################################################\n",
    "    ############       GRAPHICAL PLOTS: MODEL PERFORMANCE       #############\n",
    "    #########################################################################\n",
    "    \n",
    "    # Generate accuracy & error plots to evaluate hyperparameter choice.\n",
    "    plt.plot(alpha, train_accuracy[i-1], label = 'Training Dataset Accuracy')\n",
    "    plt.plot(alpha, test_accuracy[i-1], label = 'Testing Dataset Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Test Set_K-Fold {i}\")\n",
    "    plt.xlabel('nfolds')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(alpha, RMSE[i-1], label = 'RMSE')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Test Set_K-Fold {i}\")\n",
    "    plt.xlabel('nfolds')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    # Compile AUC data for respective ROC curves (by class).\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = len(y_encoded.columns)\n",
    "\n",
    "    for j in range(n_classes):\n",
    "        fpr[j], tpr[j], _ = roc_curve(y_test1.iloc[:, j], p_hat.iloc[:, j])\n",
    "        roc_auc[j] = auc(fpr[j], tpr[j])\n",
    "\n",
    "    # Print ROC_AUC values (per class).\n",
    "    print(\"ROC AUC Values (per class):\")\n",
    "    from pprint import pprint\n",
    "    pprint(roc_auc)\n",
    "    print()\n",
    "\n",
    "    # Plot Receiver Operating Characteristic (ROC) curves (by class).\n",
    "    for k in range(n_classes):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.step(fpr[k], tpr[k], color='black')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f\"ROC Curve (Class {k+1})\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.text(0.4, 0.6, 'AUC: {:.4f}'.format(roc_auc[k]))\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the training & validation loss curves (by class).\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.ylim(0, 40)\n",
    "    plt.xscale('log')\n",
    "    plt.plot(range(1, model.iterations+1), model.training_loss_history, label='Training Loss')\n",
    "    plt.plot(range(1, model.iterations+1), model.validation_loss_history, label='Validation Loss')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss Data\")\n",
    "    plt.title(\"Training & Validation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, 'both')\n",
    "    plt.plot(\n",
    "        [model.training_loss_history.index(min(model.training_loss_history))], [min(model.training_loss_history)], \n",
    "        marker='o', markersize=7, markerfacecolor='none', color=\"blue\")\n",
    "    plt.plot(\n",
    "        [model.validation_loss_history.index(min(model.validation_loss_history))], [min(model.validation_loss_history)], \n",
    "        marker='o', markersize=7, markerfacecolor='none', color=\"orange\")\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################\n",
    "########       K-FOLD CV MODEL PERFORMANCE AVERAGE METRICS       ########\n",
    "#########################################################################\n",
    "\n",
    "# Zip train/test accuracies & RMSE to K-hyperparameter arrays.\n",
    "k_train_accuracy = list(map(list, zip(*train_accuracy)))\n",
    "k_test_accuracy = list(map(list, zip(*test_accuracy)))\n",
    "k_RMSE = list(map(list, zip(*RMSE)))\n",
    "\n",
    "# Initialize arrays to store K-hyperparameter metrics.\n",
    "mean_k_train_accuracy = []\n",
    "mean_k_test_accuracy = []\n",
    "mean_k_RMSE = []\n",
    "\n",
    "# Compute metrics based on each hyperparameter.\n",
    "for m in range(len(alpha)):\n",
    "    mean_k_train_accuracy_temp = np.mean(k_train_accuracy[m])\n",
    "    mean_k_train_accuracy.append(mean_k_train_accuracy_temp)\n",
    "\n",
    "    mean_k_test_accuracy_temp = np.mean(k_test_accuracy[m])\n",
    "    mean_k_test_accuracy.append(mean_k_test_accuracy_temp)\n",
    "\n",
    "    mean_k_RMSE_temp = np.mean(k_RMSE[m])\n",
    "    mean_k_RMSE.append(mean_k_RMSE_temp)\n",
    "\n",
    "# Print region-based summary tables (training/testing accuracy & RMSE).\n",
    "train_accuracy_table = pd.DataFrame(train_accuracy)\n",
    "train_accuracy_table.loc['Mean (Digits)'] = train_accuracy_table.mean()\n",
    "train_accuracy_table['Mean (nFold)'] = train_accuracy_table.mean(numeric_only=True, axis=1)\n",
    "print(\"Table: Training Accuracy (5-Fold CV):\")\n",
    "print(train_accuracy_table)\n",
    "print()\n",
    "\n",
    "test_accuracy_table = pd.DataFrame(test_accuracy)\n",
    "test_accuracy_table.loc['Mean (Digits)'] = test_accuracy_table.mean()\n",
    "test_accuracy_table['Mean (nFold)'] = test_accuracy_table.mean(numeric_only=True, axis=1)\n",
    "print(\"Table: Testing Accuracy (5-Fold CV):\")\n",
    "print(test_accuracy_table)\n",
    "print()\n",
    "\n",
    "RMSE_table = pd.DataFrame(RMSE)\n",
    "RMSE_table.loc['Mean (Digits)'] = RMSE_table.mean()\n",
    "RMSE_table['Mean (nFold)'] = RMSE_table.mean(numeric_only=True, axis=1)\n",
    "print(\"Table: RMSE (5-Fold CV):\")\n",
    "print(RMSE_table)\n",
    "print()\n",
    "\n",
    "train_accuracy_table.to_csv('wine-softmax_train_accuracy_table.csv')\n",
    "test_accuracy_table.to_csv('wine-softmax_test_accuracy_table.csv')\n",
    "RMSE_table.to_csv('wine-softmax_RMSE_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
